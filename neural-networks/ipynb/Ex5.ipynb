{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the back-propagation algorithm, design a multi-layer perceptron that provides the nonlinear least-squares approximation to this data set. Compare your result against the least-squares model provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for NN with 5 hidden neurons\n",
      "epoch:  0  loss:  0.2530049681663513\n",
      "epoch:  1  loss:  0.13119956851005554\n",
      "epoch:  2  loss:  0.05598676949739456\n",
      "epoch:  3  loss:  0.04792167246341705\n",
      "epoch:  4  loss:  0.08654613792896271\n",
      "epoch:  5  loss:  0.10069407522678375\n",
      "epoch:  6  loss:  0.08351368457078934\n",
      "epoch:  7  loss:  0.058577511459589005\n",
      "epoch:  8  loss:  0.04224524646997452\n",
      "epoch:  9  loss:  0.03872436657547951\n",
      "epoch:  10  loss:  0.04393796995282173\n",
      "epoch:  11  loss:  0.05131987854838371\n",
      "epoch:  12  loss:  0.05584060773253441\n",
      "epoch:  13  loss:  0.05532652139663696\n",
      "epoch:  14  loss:  0.05012912675738335\n",
      "epoch:  15  loss:  0.042244330048561096\n",
      "epoch:  16  loss:  0.034366361796855927\n",
      "epoch:  17  loss:  0.028933808207511902\n",
      "Train loss:  0.028933808207511902\n",
      "Test loss :  0.010956122539937496\n",
      "Proposed loss :  76.11624235046557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import cm\n",
    "\n",
    "SEED = 5218\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "TRAIN_DATA_RATIO = 0.8\n",
    "\n",
    "ages = [15, 15, 15, 18, 28, 29, 37, 37, 44, 50, 50, 60, 61, 64, 65, 65, 72, 75, 75, 82, 85, 91, 91, 97, 98, 125, 142,\n",
    "        142, 147, 147, 150, 159, 165, 183, 192, 195, 218, 218, 219, 224, 225, 227, 232, 232, 237, 246, 258, 276, 285,\n",
    "        300, 301, 305, 312, 317, 338, 347, 354, 357, 375, 394, 513, 535, 554, 591, 648, 660, 705, 723, 756, 768, 860,\n",
    "        ]\n",
    "\n",
    "weights = [21.66, 22.75, 22.3, 31.25, 44.79, 40.55, 50.25, 46.88, 52.03, 63.47, 61.13, 81, 73.09, 79.09, 79.51, 65.31,\n",
    "           71.9, 86.1, 94.6, 92.5, 105, 101.7, 102.9, 110, 104.3, 134.9, 130.68, 140.58, 155.3, 152.2, 144.5, 142.15,\n",
    "           139.81, 153.22, 145.72, 161.1, 174.18, 173.03, 173.54, 178.86, 177.68, 173.73, 159.98, 161.29, 187.07,\n",
    "           176.13, 183.4, 186.26, 189.66, 186.09, 186.7, 186.8, 195.1, 216.41, 203.23, 188.38, 189.7, 195.31, 202.63,\n",
    "           224.82, 203.3, 209.7, 233.9, 234.7, 244.3, 231, 242.4, 230.77, 242.57, 232.12, 246.7,\n",
    "           ]\n",
    "data_size = len(ages)\n",
    "\n",
    "\n",
    "def test_divider(data):\n",
    "    return int(TRAIN_DATA_RATIO * len(data))\n",
    "\n",
    "\n",
    "def proposed_model_func(x):\n",
    "    return 233.846 * (1 - np.exp(-0.006042 * x))\n",
    "\n",
    "\n",
    "ages_processed = np.interp(ages, (np.min(ages), np.max(ages)), (0, 1))\n",
    "weights_processed = np.interp(weights, (np.min(weights), np.max(weights)), (0, 1))\n",
    "x_raw_train = np.reshape(ages_processed[:test_divider(ages_processed)], (test_divider(ages_processed), 1))\n",
    "y_raw_train = np.reshape(weights_processed[:test_divider(weights_processed)], (test_divider(weights_processed), 1))\n",
    "\n",
    "x_raw_test = np.reshape(ages_processed[test_divider(ages_processed):], (data_size - test_divider(ages_processed), 1))\n",
    "y_raw_test = np.reshape(weights_processed[test_divider(weights_processed):],\n",
    "                        (data_size - test_divider(weights_processed), 1))\n",
    "\n",
    "\n",
    "def main(number_of_neurons):\n",
    "    print(f'Running for NN with', number_of_neurons, 'hidden neurons')\n",
    "    # Defining input size, hidden layer size, output size and batch size respectively\n",
    "    n_in, n_h, n_out, batch_size = 1, number_of_neurons, 1, 5000\n",
    "\n",
    "    # Create training data\n",
    "    x_train = torch.FloatTensor(x_raw_train)\n",
    "    y_train = torch.FloatTensor(y_raw_train)\n",
    "\n",
    "    # Create test data\n",
    "    x_test = torch.FloatTensor(x_raw_test)\n",
    "    y_test = torch.FloatTensor(y_raw_test)\n",
    "\n",
    "    # Create the first model\n",
    "    model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(n_h, n_out),\n",
    "                          nn.ReLU())\n",
    "\n",
    "    # Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(18):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        # if epoch % 100 == 0:\n",
    "        print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred_test = model(x_test)\n",
    "    test_loss = criterion(y_pred_test, y_test)\n",
    "\n",
    "    print('Train loss: ', loss.item())\n",
    "    print('Test loss : ', test_loss.item())\n",
    "\n",
    "    y_proposed = np.array(list(map(proposed_model_func, ages)))\n",
    "    proposed_loss = np.mean((np.array(weights) - y_proposed) ** 2)\n",
    "    print('Proposed loss : ', proposed_loss)\n",
    "\n",
    "\n",
    "main(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use BP neural networks with variant sizes to illustrate the difference of over-fitting and under-fitting problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for NN with 1 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 2 hidden neurons, 18 epochs\n",
      "Train loss:  0.0673912987112999\n",
      "Test loss :  0.2606081962585449\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 3 hidden neurons, 18 epochs\n",
      "Train loss:  0.005880116950720549\n",
      "Test loss :  0.7093053460121155\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 4 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 5 hidden neurons, 18 epochs\n",
      "Train loss:  0.014253348112106323\n",
      "Test loss :  0.15194392204284668\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 6 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 7 hidden neurons, 18 epochs\n",
      "Train loss:  0.021150724962353706\n",
      "Test loss :  0.03227320685982704\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 8 hidden neurons, 18 epochs\n",
      "Train loss:  0.056369684636592865\n",
      "Test loss :  0.047282591462135315\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 9 hidden neurons, 18 epochs\n",
      "Train loss:  0.007673019077628851\n",
      "Test loss :  0.05893898382782936\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 10 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8164228200912476\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 11 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 12 hidden neurons, 18 epochs\n",
      "Train loss:  0.04140995070338249\n",
      "Test loss :  0.027929158881306648\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 13 hidden neurons, 18 epochs\n",
      "Train loss:  0.009511143900454044\n",
      "Test loss :  0.20912382006645203\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 14 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 15 hidden neurons, 18 epochs\n",
      "Train loss:  0.014298706315457821\n",
      "Test loss :  0.11872255802154541\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 16 hidden neurons, 18 epochs\n",
      "Train loss:  0.01192496344447136\n",
      "Test loss :  0.36814484000205994\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 17 hidden neurons, 18 epochs\n",
      "Train loss:  0.04658425599336624\n",
      "Test loss :  0.05754774436354637\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 18 hidden neurons, 18 epochs\n",
      "Train loss:  0.01135195791721344\n",
      "Test loss :  0.7631905674934387\n",
      "Proposed loss :  76.11624235046557\n",
      "Running for NN with 19 hidden neurons, 18 epochs\n",
      "Train loss:  0.2699261009693146\n",
      "Test loss :  0.8172007203102112\n",
      "Proposed loss :  76.11624235046557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import cm\n",
    "\n",
    "SEED = 5218\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "TRAIN_DATA_RATIO = 0.8\n",
    "\n",
    "ages = [15, 15, 15, 18, 28, 29, 37, 37, 44, 50, 50, 60, 61, 64, 65, 65, 72, 75, 75, 82, 85, 91, 91, 97, 98, 125, 142,\n",
    "        142, 147, 147, 150, 159, 165, 183, 192, 195, 218, 218, 219, 224, 225, 227, 232, 232, 237, 246, 258, 276, 285,\n",
    "        300, 301, 305, 312, 317, 338, 347, 354, 357, 375, 394, 513, 535, 554, 591, 648, 660, 705, 723, 756, 768, 860,\n",
    "        ]\n",
    "\n",
    "weights = [21.66, 22.75, 22.3, 31.25, 44.79, 40.55, 50.25, 46.88, 52.03, 63.47, 61.13, 81, 73.09, 79.09, 79.51, 65.31,\n",
    "           71.9, 86.1, 94.6, 92.5, 105, 101.7, 102.9, 110, 104.3, 134.9, 130.68, 140.58, 155.3, 152.2, 144.5, 142.15,\n",
    "           139.81, 153.22, 145.72, 161.1, 174.18, 173.03, 173.54, 178.86, 177.68, 173.73, 159.98, 161.29, 187.07,\n",
    "           176.13, 183.4, 186.26, 189.66, 186.09, 186.7, 186.8, 195.1, 216.41, 203.23, 188.38, 189.7, 195.31, 202.63,\n",
    "           224.82, 203.3, 209.7, 233.9, 234.7, 244.3, 231, 242.4, 230.77, 242.57, 232.12, 246.7,\n",
    "           ]\n",
    "data_size = len(ages)\n",
    "\n",
    "\n",
    "def test_divider(data):\n",
    "    return int(TRAIN_DATA_RATIO * len(data))\n",
    "\n",
    "\n",
    "def proposed_model_func(x):\n",
    "    return 233.846 * (1 - np.exp(-0.006042 * x))\n",
    "\n",
    "\n",
    "ages_processed = np.interp(ages, (np.min(ages), np.max(ages)), (0, 1))\n",
    "weights_processed = np.interp(weights, (np.min(weights), np.max(weights)), (0, 1))\n",
    "x_raw_train = np.reshape(ages_processed[:test_divider(ages_processed)], (test_divider(ages_processed), 1))\n",
    "y_raw_train = np.reshape(weights_processed[:test_divider(weights_processed)], (test_divider(weights_processed), 1))\n",
    "\n",
    "x_raw_test = np.reshape(ages_processed[test_divider(ages_processed):], (data_size - test_divider(ages_processed), 1))\n",
    "y_raw_test = np.reshape(weights_processed[test_divider(weights_processed):],\n",
    "                        (data_size - test_divider(weights_processed), 1))\n",
    "\n",
    "\n",
    "def main(number_of_neurons, number_of_epoch):\n",
    "    print(f'Running for NN with', number_of_neurons, 'hidden neurons,', number_of_epoch, 'epochs')\n",
    "    # Defining input size, hidden layer size, output size and batch size respectively\n",
    "    n_in, n_h, n_out, batch_size = 1, number_of_neurons, 1, 5000\n",
    "\n",
    "    # Create training data\n",
    "    x_train = torch.FloatTensor(x_raw_train)\n",
    "    y_train = torch.FloatTensor(y_raw_train)\n",
    "\n",
    "    # Create test data\n",
    "    x_test = torch.FloatTensor(x_raw_test)\n",
    "    y_test = torch.FloatTensor(y_raw_test)\n",
    "\n",
    "    # Create the first model\n",
    "    model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(n_h, n_out),\n",
    "                          nn.ReLU())\n",
    "\n",
    "    # Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(number_of_epoch):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        # if epoch % 100 == 0:\n",
    "        # print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred_test = model(x_test)\n",
    "    test_loss = criterion(y_pred_test, y_test)\n",
    "\n",
    "    print('Train loss: ', loss.item())\n",
    "    print('Test loss : ', test_loss.item())\n",
    "\n",
    "    y_proposed = np.array(list(map(proposed_model_func, ages)))\n",
    "    proposed_loss = np.mean((np.array(weights) - y_proposed) ** 2)\n",
    "    print('Proposed loss : ', proposed_loss)\n",
    "\n",
    "\n",
    "for n in range(1, 20):\n",
    "    main(n, 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast training algorithm with momentum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for NN with 5 hidden neurons, 18 epochs\n",
      "Train loss:  0.06634575128555298\n",
      "Test loss :  0.038502588868141174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import cm\n",
    "\n",
    "SEED = 5218\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "TRAIN_DATA_RATIO = 0.8\n",
    "\n",
    "ages = [15, 15, 15, 18, 28, 29, 37, 37, 44, 50, 50, 60, 61, 64, 65, 65, 72, 75, 75, 82, 85, 91, 91, 97, 98, 125, 142,\n",
    "        142, 147, 147, 150, 159, 165, 183, 192, 195, 218, 218, 219, 224, 225, 227, 232, 232, 237, 246, 258, 276, 285,\n",
    "        300, 301, 305, 312, 317, 338, 347, 354, 357, 375, 394, 513, 535, 554, 591, 648, 660, 705, 723, 756, 768, 860,\n",
    "        ]\n",
    "\n",
    "weights = [21.66, 22.75, 22.3, 31.25, 44.79, 40.55, 50.25, 46.88, 52.03, 63.47, 61.13, 81, 73.09, 79.09, 79.51, 65.31,\n",
    "           71.9, 86.1, 94.6, 92.5, 105, 101.7, 102.9, 110, 104.3, 134.9, 130.68, 140.58, 155.3, 152.2, 144.5, 142.15,\n",
    "           139.81, 153.22, 145.72, 161.1, 174.18, 173.03, 173.54, 178.86, 177.68, 173.73, 159.98, 161.29, 187.07,\n",
    "           176.13, 183.4, 186.26, 189.66, 186.09, 186.7, 186.8, 195.1, 216.41, 203.23, 188.38, 189.7, 195.31, 202.63,\n",
    "           224.82, 203.3, 209.7, 233.9, 234.7, 244.3, 231, 242.4, 230.77, 242.57, 232.12, 246.7,\n",
    "           ]\n",
    "data_size = len(ages)\n",
    "\n",
    "\n",
    "def test_divider(data):\n",
    "    return int(TRAIN_DATA_RATIO * len(data))\n",
    "\n",
    "\n",
    "def proposed_model_func(x):\n",
    "    return 233.846 * (1 - np.exp(-0.006042 * x))\n",
    "\n",
    "\n",
    "ages_processed = np.interp(ages, (np.min(ages), np.max(ages)), (0, 1))\n",
    "weights_processed = np.interp(weights, (np.min(weights), np.max(weights)), (0, 1))\n",
    "x_raw_train = np.reshape(ages_processed[:test_divider(ages_processed)], (test_divider(ages_processed), 1))\n",
    "y_raw_train = np.reshape(weights_processed[:test_divider(weights_processed)], (test_divider(weights_processed), 1))\n",
    "\n",
    "x_raw_test = np.reshape(ages_processed[test_divider(ages_processed):], (data_size - test_divider(ages_processed), 1))\n",
    "y_raw_test = np.reshape(weights_processed[test_divider(weights_processed):],\n",
    "                        (data_size - test_divider(weights_processed), 1))\n",
    "\n",
    "\n",
    "def main(number_of_neurons, number_of_epoch=18):\n",
    "    print(f'Running for NN with', number_of_neurons, 'hidden neurons,', number_of_epoch, 'epochs')\n",
    "    # Defining input size, hidden layer size, output size and batch size respectively\n",
    "    n_in, n_h, n_out, batch_size = 1, number_of_neurons, 1, 5000\n",
    "\n",
    "    # Create training data\n",
    "    x_train = torch.FloatTensor(x_raw_train)\n",
    "    y_train = torch.FloatTensor(y_raw_train)\n",
    "\n",
    "    # Create test data\n",
    "    x_test = torch.FloatTensor(x_raw_test)\n",
    "    y_test = torch.FloatTensor(y_raw_test)\n",
    "\n",
    "    # Create the first model\n",
    "    model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(n_h, n_out),\n",
    "                          nn.ReLU())\n",
    "\n",
    "    # Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(number_of_epoch):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        # if epoch % 100 == 0:\n",
    "        # print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred_test = model(x_test)\n",
    "    test_loss = criterion(y_pred_test, y_test)\n",
    "\n",
    "    print('Train loss: ', loss.item())\n",
    "    print('Test loss : ', test_loss.item())\n",
    "\n",
    "\n",
    "main(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fast training algorithm with adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for NN with 5 hidden neurons, 980 epochs\n",
      "Train loss:  0.023810975253582\n",
      "Test loss :  0.013164055533707142\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import cm\n",
    "\n",
    "SEED = 5218\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "TRAIN_DATA_RATIO = 0.8\n",
    "\n",
    "ages = [15, 15, 15, 18, 28, 29, 37, 37, 44, 50, 50, 60, 61, 64, 65, 65, 72, 75, 75, 82, 85, 91, 91, 97, 98, 125, 142,\n",
    "        142, 147, 147, 150, 159, 165, 183, 192, 195, 218, 218, 219, 224, 225, 227, 232, 232, 237, 246, 258, 276, 285,\n",
    "        300, 301, 305, 312, 317, 338, 347, 354, 357, 375, 394, 513, 535, 554, 591, 648, 660, 705, 723, 756, 768, 860,\n",
    "        ]\n",
    "\n",
    "weights = [21.66, 22.75, 22.3, 31.25, 44.79, 40.55, 50.25, 46.88, 52.03, 63.47, 61.13, 81, 73.09, 79.09, 79.51, 65.31,\n",
    "           71.9, 86.1, 94.6, 92.5, 105, 101.7, 102.9, 110, 104.3, 134.9, 130.68, 140.58, 155.3, 152.2, 144.5, 142.15,\n",
    "           139.81, 153.22, 145.72, 161.1, 174.18, 173.03, 173.54, 178.86, 177.68, 173.73, 159.98, 161.29, 187.07,\n",
    "           176.13, 183.4, 186.26, 189.66, 186.09, 186.7, 186.8, 195.1, 216.41, 203.23, 188.38, 189.7, 195.31, 202.63,\n",
    "           224.82, 203.3, 209.7, 233.9, 234.7, 244.3, 231, 242.4, 230.77, 242.57, 232.12, 246.7,\n",
    "           ]\n",
    "data_size = len(ages)\n",
    "\n",
    "\n",
    "def test_divider(data):\n",
    "    return int(TRAIN_DATA_RATIO * len(data))\n",
    "\n",
    "\n",
    "def proposed_model_func(x):\n",
    "    return 233.846 * (1 - np.exp(-0.006042 * x))\n",
    "\n",
    "\n",
    "ages_processed = np.interp(ages, (np.min(ages), np.max(ages)), (0, 1))\n",
    "weights_processed = np.interp(weights, (np.min(weights), np.max(weights)), (0, 1))\n",
    "x_raw_train = np.reshape(ages_processed[:test_divider(ages_processed)], (test_divider(ages_processed), 1))\n",
    "y_raw_train = np.reshape(weights_processed[:test_divider(weights_processed)], (test_divider(weights_processed), 1))\n",
    "\n",
    "x_raw_test = np.reshape(ages_processed[test_divider(ages_processed):], (data_size - test_divider(ages_processed), 1))\n",
    "y_raw_test = np.reshape(weights_processed[test_divider(weights_processed):],\n",
    "                        (data_size - test_divider(weights_processed), 1))\n",
    "\n",
    "\n",
    "def main(number_of_neurons, number_of_epoch=18):\n",
    "    print(f'Running for NN with', number_of_neurons, 'hidden neurons,', number_of_epoch, 'epochs')\n",
    "    # Defining input size, hidden layer size, output size and batch size respectively\n",
    "    n_in, n_h, n_out, batch_size = 1, number_of_neurons, 1, 5000\n",
    "\n",
    "    # Create training data\n",
    "    x_train = torch.FloatTensor(x_raw_train)\n",
    "    y_train = torch.FloatTensor(y_raw_train)\n",
    "\n",
    "    # Create test data\n",
    "    x_test = torch.FloatTensor(x_raw_test)\n",
    "    y_test = torch.FloatTensor(y_raw_test)\n",
    "\n",
    "    # Create the first model\n",
    "    model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "                          nn.ReLU(),\n",
    "                          nn.Linear(n_h, n_out),\n",
    "                          nn.ReLU())\n",
    "\n",
    "    # Construct the loss function\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    # Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "    optimizer = torch.optim.Adadelta(model.parameters(), lr=0.1)\n",
    "\n",
    "    # Gradient Descent\n",
    "    for epoch in range(number_of_epoch):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        # if epoch % 100 == 0:\n",
    "        # print('epoch: ', epoch, ' loss: ', loss.item())\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    y_pred_test = model(x_test)\n",
    "    test_loss = criterion(y_pred_test, y_test)\n",
    "\n",
    "    print('Train loss: ', loss.item())\n",
    "    print('Test loss : ', test_loss.item())\n",
    "\n",
    "\n",
    "main(5, 980)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
